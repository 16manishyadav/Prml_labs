# -*- coding: utf-8 -*-
"""B21CS044 LAB 3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Hn7aHa30wtD9QHcEa-ucb_NP_xtF6ZpY
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split as tts
from sklearn.linear_model import LogisticRegression as LR
from sklearn.tree import DecisionTreeClassifier as DTC
from sklearn.metrics import confusion_matrix as cm
from sklearn.metrics import precision_score as ps
from sklearn.metrics import recall_score as rs
from sklearn.metrics import f1_score as f1s
from sklearn.metrics import accuracy_score as acc
from sklearn.tree import DecisionTreeRegressor

data = pd.read_csv('titanic.csv')
data

#here visualizing passengerid and names with survived will not make any sense because these are unique identities
data.isnull().sum()
data.drop(['Name','PassengerId','Ticket','Cabin'],axis=1,inplace=True)
data.dropna(subset=['Embarked'],inplace = True)
data['Age'].fillna(data.Age.mean(),inplace=True)
import seaborn as sns
sns.countplot(data=data,x='Survived',hue='Pclass')

#here we can clearly see that class 3 %save was least



sns.countplot(data=data,x='Survived',hue='Sex')
#here we can see that male sex was least saved

#plotting between two columns
import matplotlib.pyplot as plt
sns.barplot(data=data,x='Survived',y='Age')

sns.barplot(data=data,x='Survived',y='Fare')

sns.scatterplot(data=data,x='Fare',y='Age',hue='Survived')

sns.barplot(data=data,x='Age',y='Fare')



#spilliting the data
from sklearn.model_selection import train_test_split
data = pd.get_dummies(data, columns=["Embarked","Sex"])
x = data.drop('Survived',axis=1)
y = data['Survived']
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=5)

# socre of diifrent metrices for the Naive Bayes classifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
# Train the Gaussian Naive Bayes classifier
clf = GaussianNB()
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(accuracy)
y_probs = clf.predict_proba(X_test)[:, 1]
roc_auc = roc_auc_score(y_test, y_probs)
print(roc_auc)
precision = precision_score(y_test, y_pred)
print(precision)
f1 = f1_score(y_test, y_pred)
print(f1)

# 5 flod validation for Naive Bayes classifier
from sklearn. model_selection import cross_val_score
#performing 5-fold cross validation
# we can perform the k fold validation by just giving any other number to k in
from sklearn.model_selection import KFold
# Initialize the KFold splitter
kf = KFold(n_splits=5, shuffle=True, random_state=5)
accuracy = cross_val_score(clf,x,y,cv=kf)
print(accuracy)
meanacc = accuracy.mean()
print(meanacc)



#computing probability of each row of test data of top class
probab = clf.predict_proba(X_test)
top_class = []
for i in probab:
  top_class.append(max(i))
print(top_class)

#plotting the contour plots for the three features
sns.kdeplot(x=X_test['Age'],y=X_test['Fare'],hue=y_test)

sns.kdeplot(x=X_test['Pclass'],y=X_test['Fare'],hue=y_test)

sns.kdeplot(x=X_test['Age'],y=X_test['Pclass'],hue=y_test)



from sklearn.tree import DecisionTreeClassifier
tree_1 = DecisionTreeClassifier()
tree_1.fit(X_train,y_train)

# performing 5 fold validtion for Decision Tree classifier
from sklearn. model_selection import cross_val_score
#performing 5-fold cross validation
# we can perform the k fold validation by just giving any other number to k in
from sklearn.model_selection import KFold
# Initialize the KFold splitter
kf = KFold(n_splits=5, shuffle=True, random_state=5)
accuracy = cross_val_score(tree_1,x,y,cv=kf)
print(accuracy)
meanacc = accuracy.mean()
print(meanacc)

"""## Question 2

"""

dt = pd.read_csv('dataset (1).csv')
dt

# plotting all features as histogram
sns.histplot(dt['X0'],bins=20)

sns.histplot(dt['X1'],bins=20)

sns.histplot(dt['X2'],bins=20)

sns.histplot(dt['X3'],bins=20)

sns.histplot(dt['X4'],bins=20)

sns.histplot(dt['X5'],bins=20)

sns.histplot(dt['X6'],bins=20)

# Calculating the prior probablity of each class
count1=dt['Y'].value_counts()[1]
count2=dt['Y'].value_counts()[2]
count3=dt['Y'].value_counts()[3]
total = 210
prior_porb1 = count1/total
prior_porb2 = count2/total
prior_porb3 = count3/total
print(prior_porb1)
print(prior_porb2)
print(prior_porb3)

# functions to discretize the data
def discretization(num_bins,data):
  min_val = min(data)
  max_val = max(data)
  bin_width = (max_val - min_val) / num_bins
  bins = []
  for i in range(num_bins):
      bins.append(min_val + (i + 1) * bin_width)
  discretized_data = []
  for val in data:
      for i in range(len(bins)):
          if val <= bins[i]:
              discretized_data.append(i)
              break
  return discretized_data
X_0=discretization(5,dt['X0'])
X_1=discretization(5,dt['X1'])
X_2=discretization(5,dt['X2'])
X_3=discretization(5,dt['X3'])
X_4=discretization(5,dt['X4'])
X_5=discretization(5,dt['X5'])
X_6=discretization(5,dt['X6'])
#plotting the data discrete data now
print(X_0)
print(X_1)
print(X_2)
print(X_3)
print(X_4)
print(X_5)
print(X_6)
df = pd.DataFrame(X_0,columns=['X_0'])
df['X_1'] = X_1
df['X_2'] = X_2
df['X_3'] = X_3
df['X_4'] = X_4
df['X_5'] = X_5
df['X_6'] = X_6
df['Y'] =dt['Y']
df

plt.hist(X_0,color='blue')

plt.hist(X_1,color='blue')

plt.hist(X_2,color='blue')

plt.hist(X_3,color='blue')

plt.hist(X_4,color='blue')

plt.hist(X_5,color='blue')

plt.hist(X_6,color='blue')

y1 = df['Y'].to_numpy()
print(y1)

# the likelihood/class conditional probabilities
def countofxandy(x,y,bin,cls):
  count = 0
  for i in range(len(x)):
      if(x[i]==bin and y[i]==cls):
        count+=1
  return count
def lihood(x,y,bin,cls,ct_cls):
  p=(countofxandy(x,y,bin,cls))/ct_cls
  return p
clss1 = []
clss2 = []
clss3 = []
for i in range(1,4):
  for j in range(5):
    if(i==1):
      p = lihood(X_0,y1,i,j,70)*lihood(X_1,y1,i,j,70)*lihood(X_2,y1,i,j,70)*lihood(X_3,y1,i,j,70)*lihood(X_4,y1,i,j,70)*lihood(X_5,y1,i,j,70)*lihood(X_6,y1,i,j,70)
      clss1.append(p)
    if(i==2):
      p = lihood(X_0,y1,i,j,70)*lihood(X_1,y1,i,j,70)*lihood(X_2,y1,i,j,70)*lihood(X_3,y1,i,j,70)*lihood(X_4,y1,i,j,70)*lihood(X_5,y1,i,j,70)*lihood(X_6,y1,i,j,70)
      clss2.append(p)
    if(i==3):
      p = lihood(X_0,y1,i,j,70)*lihood(X_1,y1,i,j,70)*lihood(X_2,y1,i,j,70)*lihood(X_3,y1,i,j,70)*lihood(X_4,y1,i,j,70)*lihood(X_5,y1,i,j,70)*lihood(X_6,y1,i,j,70)
      clss3.append(p)
print(clss1)
print(clss2)
print(clss3)



sns.countplot(data=df,x='X_0',hue='Y')

sns.countplot(data=df,x='X_1',hue='Y')

sns.countplot(data=df,x='X_2',hue='Y')

sns.countplot(data=df,x='X_3',hue='Y')

sns.countplot(data=df,x='X_4',hue='Y')

sns.countplot(data=df,x='X_5',hue='Y')

sns.countplot(data=df,x='X_6',hue='Y')



#posterior = likelihood*prior/evidence
# if we consider them as independent evidence = 1/7 and prior = 1/3
for i in range(5):
  clss1[i] = (clss1[i]*7)/3
  clss2[i] = (clss2[i]*7)/3
  clss3[i] = (clss3[i]*7)/3
print(clss1)
print(clss2)
print(clss3)

#because maximum range of all features is from 0 to 20 that's why we are using linspace range form 0 to 20 and bins = 5 thats why bins =5
plt.plot(np.linspace(0,20,5),clss1,label='1')
#here 1 is representing class 1
plt.plot(np.linspace(0,20,5),clss2,label='2')
#here 2 is representing class 2
plt.plot(np.linspace(0,20,5),clss3,label='3')
# here 3 is representing class 3
plt.legend()
plt.xlabel('All Features')
plt.ylabel('Posterior Probablity Classwise')
plt.show()