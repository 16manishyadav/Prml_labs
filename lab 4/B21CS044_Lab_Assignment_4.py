# -*- coding: utf-8 -*-
"""B21CS044_Lab_Assignment_4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_dpoKw4TK_VxPbE_tZewNkJ-ybrlzBBm

## QUESTION 2
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split as tts
from sklearn.linear_model import LogisticRegression as LR
from sklearn.tree import DecisionTreeClassifier as DTC
from sklearn.metrics import confusion_matrix as cm
from sklearn.metrics import precision_score as ps
from sklearn.metrics import recall_score as rs
from sklearn.metrics import f1_score as f1s
from sklearn.metrics import accuracy_score as acc
from sklearn.tree import DecisionTreeRegressor



import matplotlib.pyplot as plt
covariance = [[3/2,1/2],[1/2,3/2]]
mean = [0,0]
X_data = np.random.multivariate_normal(mean,covariance,200).T
covariance_matrix = np.cov(X_data)
print(covariance_matrix)
#finding eigenvalues and eigenvectors
e_values,e_vectors = np.linalg.eig(covariance_matrix) 
print(e_values)
print(e_vectors)
plt.plot(e_values,e_vectors)
plt.scatter(X_data[0:200][0],X_data[0:200][1])
plt.xlabel('eigenvalues')
plt.ylabel('eigenvectors')
plt.show()



"""## part b"""



import scipy as sp
power = -0.5
covariance_inversed=sp.linalg.fractional_matrix_power(covariance_matrix, power)
print(covariance_inversed)
Y_data = np.matmul(covariance_inversed,X_data)
from sklearn.naive_bayes import GaussianNB
print(Y_data)
Y_covmat = np.cov(Y_data)
print(Y_covmat)



"""## the purpose of doing so is to make the features independent as their covarianve will be 0 or tending to 0"""

import seaborn as sns
points = 10
angle = np.linspace(0, 2 * np.pi, points)
r = 5
x1 = r * np.cos(angle)
y1 = r * np.sin(angle)
P_data = np.array([x1,y1])
sns.scatterplot(x=X_data[0:200][0],y=X_data[0:200][1])
sns.scatterplot(x=x1, y=y1,hue=[0,1,2,3,4,5,6,7,8,9])
plt.title('Data points with samples')
plt.xlabel('x')
plt.ylabel('y')
plt.show()

"""## since the u = 0,0 so we dont have to minus anything else"""

import math as mt
euclidean_distance = []
points = [1,2,3,4,5,6,7,8,9,10]
for i in range(len(x1)):
    euclidean_distance.append(mt.sqrt(x1[i]**2+y1[i]**2))
sns.barplot(y=euclidean_distance,x=points,width=0.5)
plt.show()
print(euclidean_distance)

power = -0.5
covariance_inversed=sp.linalg.fractional_matrix_power(covariance_matrix, power)
print(covariance_inversed)
Q_data = np.matmul(covariance_inversed,P_data)
print(Q_data)

sns.scatterplot(x=Y_data[0:200][0],y=Y_data[0:200][1])
sns.scatterplot(x=Q_data[0], y=Q_data[1],hue=[0,1,2,3,4,5,6,7,8,9])
plt.title('Data points with samples')
plt.xlabel('x')
plt.ylabel('y')
plt.show()

# plot of barplot for the euclidiean distance of the points in Q_data with 0,0
euclidean_distance = []
for i in range(len(Q_data[0])):
    euclidean_distance.append(mt.sqrt(Q_data[0][i]**2+Q_data[1][i]**2))
sns.barplot(x=points,y=euclidean_distance,width=0.5)
plt.show() 
print(euclidean_distance)

"""Question 1"""

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
data1 = pd.read_csv('iris.data')
X_data = data1.drop(['Class'],axis=1)
from mlxtend.plotting import plot_decision_regions
y_data = data1['Class']
label_encoder = LabelEncoder()
y_data = label_encoder.fit_transform(y_data)
#convert y_data to dataframe
# y_data = pd.DataFrame(y_data,columns=['Class'])
X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.3, random_state=42)
X_data

sns.scatterplot(data=data1,x='Sepal length',y='Sepal width',hue=data1['Class'])

sns.scatterplot(data=data1,x='Petal length',y='Petal width',hue=data1['Class'])

corr = data1.iloc[:,:-1].corr(method="pearson")
cmap = sns.diverging_palette(250,354,80,60,center='dark',as_cmap=True)
sns.heatmap(corr, vmax=1, vmin=-.5, cmap=cmap, square=True, linewidths=.2)

fig, axes = plt.subplots(1, 2, figsize=(18, 6), sharey=True)
sns.histplot(data1, ax=axes[0], x="Sepal length", kde=True, color='r')
sns.histplot(data1, ax=axes[1], x="Sepal width", kde=True, color='b')

fig, axes = plt.subplots(1, 2, figsize=(18, 6), sharey=True)
sns.histplot(data1, ax=axes[0], x="Petal length", kde=True,color='g')
sns.histplot(data1, ax=axes[1], x="Petal width", kde=True,color='r')

class GaussianNBC:
    def __init__(self,type):
        self.type=type
        self.data = {}

    def mean_column(self,X,Y):
        cls = np.unique(Y)
        length = len(cls)
        X = X.to_numpy()
        mean_arr = np.zeros(length)
        for i in range(length):
            meanval = 0
            total = 0
            for j in range(len(X)):
                if Y[j]==cls[i]:
                    meanval += X[j]
                    total += 1
            meanval = meanval/total
            mean_arr[i] = meanval
        return mean_arr
    def mean_all_features(self,X,Y):
        mean_arr_all = np.zeros([len(X.axes[1]),3])
        for column in X:
            mean_arr = self.mean_column(X[column],Y)
            index = X.columns.get_loc(column)
            mean_arr_all[index][0:3] = mean_arr
        return mean_arr_all
    def get_mean_class(self,X,Y,cls):
        meanall = self.mean_all_features(X,Y)
        return meanall[:,cls]
    def covariance_class(self,X,Y,i):
        X_1 = X[Y==i]
        cov_mat = np.cov(X_1.T)
        diag_value = cov_mat[0,0]
        other_value = cov_mat[0,1]
        if(self.type=='type1'):
            for i in range(cov_mat.shape[0]):
                for j in range(cov_mat.shape[1]):
                    if(i==j):
                        cov_mat[i,j] = diag_value
                    else:
                        cov_mat[i,j] = 0
        if(self.type=='type2'):
            for i in range(cov_mat.shape[0]):
                for j in range(cov_mat.shape[1]):
                    if(i==j):
                        cov_mat[i,j] = diag_value
                    else:
                        cov_mat[i,j] = other_value
        if(self.type=='type3'):
            cov_mat = cov_mat
        return cov_mat
    def calculate_prior(self,Y):
        unique, counts = np.unique(Y, return_counts=True)
        prior = counts / np.sum(counts)
        return prior
    def train(self,X,Y):
        cls = np.unique(Y)
        for i in range(len(cls)):
            prior = self.calculate_prior(Y)[cls[i]]
            # print(prior)
            cov_mat = self.covariance_class(X,Y,cls[i])
            # print(cov_mat)
            mean_cls = self.get_mean_class(X,Y,cls[i])
            # print(mean_cls)
            self.data[cls[i]] = {'prior':prior,'cov_mat':cov_mat,'mean_cls':mean_cls}
    
    def discriminant(self,X_test,Y_test):
        discriminant_cls = np.unique(Y_test)
        row = X_test.T
        row = row.to_numpy()
        for i in range(len(discriminant_cls)):
            x_u = row-self.data[discriminant_cls[i]]['mean_cls']
            inverse_cov = np.linalg.inv(self.data[discriminant_cls[i]]['cov_mat'])
            x_u_t = x_u.T
            discriminant_cls[i] = np.dot(np.matmul(x_u_t,inverse_cov),x_u)*(-0.5)+np.log(self.data[discriminant_cls[i]]['prior'])+(-0.5)*np.log(np.linalg.det(self.data[discriminant_cls[i]]['cov_mat']))
        return discriminant_cls
    def predict(self,X_point,y_test):
        discriminant_cls = self.discriminant(X_point,y_test)
        return np.argmax(discriminant_cls)
    def test(self,X_test,y_test):
        y_pred = np.zeros(len(X_test))
        for i in range(len(X_test)):
            y_pred[i] = self.predict(X_test.iloc[i],y_test)
        accuracy = 0
        for i in range(len(y_pred)):
            if y_pred[i] == y_test[i]:
                accuracy += 1
        accuracy = accuracy/len(y_pred)
        return accuracy, y_pred
dt1 = GaussianNBC('type1')
dt1.train(X_train,y_train)
print(dt1.test(X_test,y_test))

dt2 = GaussianNBC('type2')
dt2.train(X_train,y_train)
print(dt2.test(X_test,y_test))

dt3 = GaussianNBC('type3')
dt3.train(X_train,y_train)
print(dt3.test(X_test,y_test))   

def plot_decision_region(X_train,y_train,type,resolution=45):
        model = GaussianNBC(type)
        model.train(X_train,y_train)
        X1 = X_train.iloc[:,0]
        X2 = X_train.iloc[:,1]
        X1 = X1.to_numpy()
        X2 = X2.to_numpy()
    # Get the range of values for the two features
        x1_min, x1_max = X1.min(), X1.max()
        x2_min, x2_max = X2.min(), X2.max()

        # Create a grid of points to evaluate the model at
        xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max, resolution),
                            np.linspace(x2_min, x2_max, resolution))
        X_grid = np.column_stack((xx1.ravel(), xx2.ravel()))
        # Make predictions for the grid of points
        X_grid = pd.DataFrame(X_grid)
        acc,y_grid = model.test(X_grid,y_test)

        # Reshape the predictions to match the grid shape
        y_grid = y_grid.reshape(xx1.shape[0])

        # Plot the decision boundary and the training data
        plt.contourf(xx1, xx2, y_grid, cmap=plt.cm.Paired)
        plt.scatter(X1, X2, c=y, cmap=plt.cm.Paired, edgecolors='k')
        plt.xlabel(f"Feature1")
        plt.ylabel(f"Feature2")
        plt.show()

X_train1 = X_train.iloc[:,0:2]
X_train1 = X_train1.to_numpy()

model1 = GaussianNB()
model1.fit(X_train1,y_train)
plot_decision_regions(X_train1,y_train,clf=model1)
plt.xlabel('Petal width')
plt.ylabel('Petal length')
plt.title('Type1')
plt.show()

model2 = GaussianNB()
model2.fit(X_train1,y_train)
plot_decision_regions(X_train1,y_train,clf=model2)
plt.xlabel('Petal width')
plt.ylabel('Petal length')
plt.title('Type2')
plt.show()

model3 = GaussianNB()
model3.fit(X_train1,y_train)
plot_decision_regions(X_train1,y_train,clf=model3)
plt.xlabel('Petal width')
plt.ylabel('Petal length')
plt.title('Type3')
plt.show()

from sklearn. model_selection import cross_val_score
from sklearn.model_selection import KFold
# Initialize the KFold splitter
kf = KFold(n_splits=5, shuffle=True, random_state=42)
accuracy = cross_val_score(model1,X_train1,y_train,cv=kf)
print(accuracy)
meanacc = accuracy.mean()
print(meanacc)

kf = KFold(n_splits=5, shuffle=True, random_state=20)
accuracy = cross_val_score(model2,X_train1,y_train,cv=kf)
print(accuracy)
meanacc = accuracy.mean()
print(meanacc)

kf = KFold(n_splits=5, shuffle=True, random_state=5)
accuracy = cross_val_score(model2,X_train1,y_train,cv=kf)
print(accuracy)
meanacc = accuracy.mean()
print(meanacc)

angles = np.random.uniform(0, 2*np.pi, size=500)
radi = np.random.uniform(0, 5, size=500)
x = radi * np.cos(angles)
y = radi * np.sin(angles)
distances = np.sqrt(x**2 + y**2)
labels = np.where(distances <= 3, 1, np.where(distances <= 5, 2, 0))

plt.scatter(x[labels==1], y[labels==1], color='green', label='Class 1')
plt.scatter(x[labels==2], y[labels==2], color='red', label='Class 2')
plt.title('Datapoints')
plt.show()

model3 = GaussianNB()
X = np.column_stack((x, y))
model3.fit(X, labels)

x1, y1 = np.meshgrid(np.linspace(-6, 6, 100), np.linspace(-6, 6, 100))
Z = model3.predict(np.c_[x1.ravel(), y1.ravel()])
Z = Z.reshape(x1.shape)
plt.scatter(x[labels==1], y[labels==1], color='green', label='Class 1')
plt.scatter(x[labels==2], y[labels==2], color='red', label='Class 2')
plt.contour(x1, y1, Z, colors='k', levels=[1.5], linestyles=['--'])
plt.title('Datapoints and decision boundary')
plt.show()

