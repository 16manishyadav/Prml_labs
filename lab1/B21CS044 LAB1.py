# -*- coding: utf-8 -*-
"""Untitled18.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1E_YSN2QSyznLRljNiBF2szv-yX212bzP

##Q1
"""

file = open("f1.txt")
#f2 = open("Things to do.docx")
f1 = []
for i in file:
  f1.append(i.strip())
print(f1)

x = input()
print(type(x))
a = int(x)
print(type(a))

import datetime
dt = "10-01-2023 15:51:35"
dtfile = datetime.datetime.strptime(dt,"%d-%m-%Y %H:%M:%S")
print(dtfile)
print(type(dtfile))

import os
#os.system("f1.txt")
os.times()

l1 = [1,2,3,4,5,6,7,8,9,1,2,1,2,3,1,5,6,1,7,9,1]
#assume we have to count 1
count = l1.count(1)
print(count)

from functools import reduce
from operator import concat
l1 = [[1,232,5],["a","b","c"],[1.10,1.20,1.22]]
lst = reduce(concat,l1)
print(lst)

d1 = {"a":1,"b":10,"c":100}
d2 = {"e":1000,"f":1234,1:12359}
print({**d1,**d2})

l1 = [1,1,1,2,13,5,1,1,1,1,]
q = set(l1)
lst = list(q)
print(lst)

d1 = {1:1,2:2,3:3,4:4}
k = int(input())
flag = False
for i in d1:
  if(k==i):
    flag = True
    print("yes")
    break
if(flag==False):
  print("no")

"""##Q2"""

import numpy as np
l1 = np.arange(5,9).reshape(2,2)
l2 = np.arange(10,14).reshape(2,2)
print(l1[0])
print(l2[:,1])
print(np.matmul(l1,l2))
print(np.multiply(l1,l2))
for i in range(2):
  for j in range(2):
    print(np.dot(l1[:,i],l2[:,j]))

"""##Q3"""

import pandas as pd
import numpy as np
# Model : Nominal
# Type : Ordinal
# Max Price : ratio
# Airbags : Ordninal

def f(df1):
  df1.fillna(df1.mean(), inplace=True)
  return df1

# reducing noice
import pandas as pd
import numpy as np
def f(df,windsize):
  df1 = df.rolling(windsize).mean()
  return df1

# fuction to nomalise or scale features
import pandas as pd
import numpy as np
#using the fuction data-data.min()/(data.max()-data.min()) to normalize the features

def normalize(df, method):
    if method == 'joint':
        dfscale = (df - df.min()) / (df.max() - df.min())
    else:
        dfscale = df.copy()
        for col in df.columns:
            dfscale[col] = (df[col] - df[col].min()) / (df[col].max() - df[col].min())
    return dfscale

# function to create a random split
import numpy as np
import pandas as pd

def f(x):
  np.random.shuffle(x)
  x1 = x[:70]
  x2 = x[70:90]
  x3 = x[70::]
  print(x1,x2,x3)
  return
x = np.arange(start = 1,stop = 101,step = 1)
f(x)

# encoding as categorical variables
import numpy as np
import pandas as pd
dict1 = {'First Score':[110, 95, np.nan, 90],
        'Second Score': [35, 423, 59, np.nan],
        'Third Score':[np.nan, 4, 88, 99],
    'gender':['male','transgender','female',np.nan]}
df1 = pd.DataFrame(dict1)

def encoding(df):
    catecols = df.select_dtypes(include=['object','category']).columns
    encodedarr = pd.get_dummies(df, columns=catecols)
    return encodedarr
print(encoding(df1));

"""##Q4"""

import numpy as np
import matplotlib.pyplot as plt
x = np.linspace(-10,10)
y = 5*x+4
plt.plot(x,y,color='red')
plt.show()

x1 = np.linspace(10,100)
y1 = np.log(x1)
plt.plot(x1,y1,color='blue')
plt.show()

x2 = np.linspace(-10,10)
y2 = x2*x2
plt.plot(x2,y2,color='purple')
plt.show()

x3 = np.array([0,1,2,3,4])
y3 = np.array([2,3,4,5,6])
plt.scatter(x3,y3,color='pink')
plt.show()

"""##Q5

# Import the Necessary Python Libraries and Components
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split as tts
from sklearn.linear_model import LogisticRegression as LR
from sklearn.tree import DecisionTreeClassifier as DTC
from sklearn.metrics import confusion_matrix as cm
from sklearn.metrics import precision_score as ps
from sklearn.metrics import recall_score as rs
from sklearn.metrics import f1_score as f1s
from sklearn.metrics import accuracy_score as acc

"""### To Disable Convergence Warnings (For Custom Training)

"""

from warnings import simplefilter
from sklearn.exceptions import ConvergenceWarning
simplefilter("ignore", category=ConvergenceWarning)

"""# 1.) Input the Dataset"""

# Dataset Reference :- https://www.kaggle.com/uciml/breast-cancer-wisconsin-data

data = pd.read_csv('/content/data.csv')
data

"""# 2.) Convert the String Labels into easily-interpretable Numerics"""

# Note :- There are many existing Encoders for converting String to Numeric Labels, but for convenience, we used Pandas.

condition_M = data.diagnosis == "M"
condition_B = data.diagnosis == "B"

data.loc[condition_M,"diagnosis"]=0
data.loc[condition_B,"diagnosis"]=1

data

"""# 3.) Converting Dataframe into Numpy Arrays (Features and Labels)"""

Y = data.diagnosis.to_numpy().astype('int')                                     # Labels

X_data = data.drop(columns=["id","diagnosis","Unnamed: 32"])
X = X_data.to_numpy()                                                           # Input Features

"""# 4.) Splitting the Dataset into Train and Test Portions"""

user_prompt = 0.3
user_enable = False

x_train,x_test,y_train,y_test = tts(X,Y,test_size=user_prompt,shuffle=user_enable)

"""# 5.) Model Training and Predicting"""

# Note :- Don't worry about the code snippet here, it is just to produce the predictions for the test data portion of each classifier

logistic_model = LR()
logistic_model.fit(x_train,y_train)
logistic_pred = logistic_model.predict(x_test)

decision_model = DTC()
decision_model.fit(x_train,y_train)
decision_pred = decision_model.predict(x_test)

"""# 6.) Evaluation Metrics (Inbulit v/s Scaratch)"""

inbuilt_matrix_logistic = cm(y_test,logistic_pred)
inbuilt_matrix_decision = cm(y_test,decision_pred)

print("Confusion Matrix for Logistic Regression-based Predictions =>")
print(inbuilt_matrix_logistic)
print("Confusion Matrix for Decision Tree-based Predictions =>")
print(inbuilt_matrix_decision)

def confusion_matrix(y_test,logistic_pred,decision_pred):
  confmat1 = np.zeros((2,2),dtype=np.int16)
  confmat2 = np.zeros((2,2),dtype=np.int16)
  for i in range(len(y_test)):
      confmat1[y_test[i]][logistic_pred[i]]+=1
  for i in range(len(y_test)):
      confmat2[y_test[i]][decision_pred[i]]+=1
  return confmat1,confmat2

a,b=confusion_matrix(y_test,logistic_pred,decision_pred)
print(a)
print(b)

"""## Average Accuracy"""

inbuilt_acc_logistic = acc(y_test,logistic_pred)
inbuilt_acc_decision = acc(y_test,decision_pred)

print("Accuracy for Logistic Regression-based Predictions =>",str(inbuilt_acc_logistic*100)+"%")
print("Accuracy for Decision Tree-based Predictions =>",str(inbuilt_acc_decision*100)+"%")

def avg_accuracy():
  a,b=confusion_matrix(y_test,logistic_pred,decision_pred)
  logistic_avg = (a[0][0]+a[1][1])/(a[0][0]+a[1][1]+a[1][0]+a[0][1])
  decision_avg = (b[0][0]+b[1][1])/(b[0][0]+b[1][1]+b[1][0]+b[0][1])
  return logistic_avg,decision_avg
a1,b1 = avg_accuracy()
print(a1*100)
print(b1*100)

"""## Precision"""

inbuilt_ps_logistic = ps(y_test,logistic_pred)
inbuilt_ps_decision = ps(y_test,decision_pred)

print("Precision for Logistic Regression-based Predictions =>",str(inbuilt_ps_logistic*100)+"%")
print("Precision for Decision Tree-based Predictions =>",str(inbuilt_ps_decision*100)+"%")

def precision():
  a,b=confusion_matrix(y_test,logistic_pred,decision_pred)
  log_pre = a[1][1]/(a[1][1]+a[0][1])
  dec_pre = b[1][1]/(b[1][1]+b[0][1])
  return log_pre,dec_pre
pre1,pre2 = precision()
print(pre1*100)
print(pre2*100)

"""## Recall"""

inbuilt_rs_logistic = rs(y_test,logistic_pred)
inbuilt_rs_decision = rs(y_test,decision_pred)

print("Recall for Logistic Regression-based Predictions =>",str(inbuilt_rs_logistic*100)+"%")
print("Recall for Decision Tree-based Predictions =>",str(inbuilt_rs_decision*100)+"%")

def recall():
  rec1 = a[1][1]/(a[1][1]+a[1][0])
  rec2 = b[1][1]/(b[1][1]+b[1][0])
  return rec1,rec2
rec1,rec2 = recall()
print(rec1*100)
print(rec2*100)

"""## F-1 Score"""

inbuilt_f1s_logistic = f1s(y_test,logistic_pred)
inbuilt_f1s_decision = f1s(y_test,decision_pred)

print("F1-Score for Logistic Regression-based Predictions =>",str(inbuilt_f1s_logistic*100)+"%")
print("F1-Score for Decision Tree-based Predictions =>",str(inbuilt_f1s_decision*100)+"%")

def f1_score():
  f11 = 2/((1/pre1)+(1/rec1))
  f22 = 2/((1/pre2)+(1/rec2))
  return f11,f22
f11,f22 = f1_score()
print(f11*100)
print(f22*100)

"""## Class-Wise Accuracy"""

def class_accuracy():
  c_acc1 = (pre1+rec1)/2
  c_acc2 = (pre2+rec2)/2
  return c_acc1,c_acc2
c_acc1,c_acc2 = class_accuracy()
print(c_acc1*100)
print(c_acc2*100)

"""## Sensitivity"""

def sensitivity():
  sen1 = rec1
  sen2 = rec2
  return rec1,rec2
sen1,sen2 = sensitivity()
print(sen1*100)
print(sen2*100)

"""## Specificity"""

def specificity():
  spe1 = a[0][0]/(a[0][0]+a[0][1])
  spe2 = b[0][0]/(b[0][0]+b[0][1])
  return spe1,spe2
spe1,spe2 = specificity()
print(spe1*100)
print(spe2*100)